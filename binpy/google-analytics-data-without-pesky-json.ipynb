{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Analytics Customer Revenue Prediction Data Rewrite\n",
    "*NOTE: now using v2 data*\n",
    "\n",
    "This kernel is merely show how to convert the train.csv and test.csv from large cumbersome with JSON columns to the lighter newtrain.csv and newtest.csv, where the nested JSON entries have been expanded (flattened) into extra columns. Since all JSON field name repetition is removed the resultant files are approximately 1GB lighter (eg. 1.5GB to 250MG for train.csv). Though no information has been lost, the resultant files are in UTF-8 format instead of ASCII.\n",
    "\n",
    "This kernel is set up to do the conversion from original data to new data and write it to your working directory, even though dataset **\"GA data with json columns\"** is already included (it purposefully fails to find the new dataset and proceeds to load and convert the original one). To change this behaviour and load the new dataset immediately uncomment the line  `newdata_dir = \"../input/ga-analytics-with-json-columns\"` in the first code cell.\n",
    "\n",
    "#### dtypes/type changes after reloading data\n",
    "It is worth noting that a reload of the manipulated data (eg. `newtrain = pd.read_csv(newtrain_path)`) assigns more appropriate dtypes and types (to columns and elements respectively) than the first csv read. The sequence is (1) read original csv into DataFrame `train`; (2) which is then copied to `newtrain`; (3) JSON fields are added; (4) `newtrain` is written to a new csv; (5) `newtrain` is reloaded from the new csv. Only after (5) is done do we see appropriate values for many columns.\n",
    "* Eg: `totals.transactionRevenue` starts out life as an **object** with elements of type **&lt;class 'float'>**, but once reloaded the dtype and type change to **float64** and **&lt;class 'numpy.float'>** respectively. The latter change was important when using `pd.sum()`.\n",
    "\n",
    "#### dtype of fullVisitorId\n",
    "Many `fullVisitorId`s have leading zeros, which are stripped when a csv is read without forcing the dtype(str). Since the example submission contains leading zeros we suppose we must keep the zeros.\n",
    "\n",
    "#### extra stuff\n",
    "After the reload, there is some extra stuff which doesn't affect the new cvs such as dropping columns filled only with NaNs. Also, there some rudimentary EDA pies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data overview\n",
    "\n",
    "_NOTE: Jupyter has removed support for :--- to left justify so have had to use `<p align=\"left\">`_<br>\n",
    "_NOTE: JSON indicates many more columns will be added once these columns with json fields have been expanded_\n",
    "\n",
    "Field|<p align=\"left\">Description\n",
    "---|:---\n",
    " fullVisitorId        | <p align=\"left\">A unique identifier for each user of the Google Merchandise Store\n",
    " channelGrouping      |<p align=\"left\"> The channel via which the user came to the Store\n",
    " date                 |<p align=\"left\"> The date on which the user visited the Store\n",
    " device               |<p align=\"left\"> (JSON) The specifications for the device used to access the Store\n",
    " geoNetwork           |<p align=\"left\"> (JSON) This section contains information about the geography of the user\n",
    " sessionId            |<p align=\"left\"> A unique identifier for this visit to the store\n",
    " socialEngagementType |<p align=\"left\"> Engagement type, either \"Socially Engaged\" or \"Not Socially Engaged\"\n",
    " totals               |<p align=\"left\"> (JSON) This section contains aggregate values across the session\n",
    " trafficSource        |<p align=\"left\"> (JSON) This section contains information about the Traffic Source from which the session originated. \n",
    " visitId              |<p align=\"left\"> An identifier for this session. This is part of the value usually stored as the `_utmb` cookie, uique only to user. For unique visit ID use: fullVisitorId and visitId\n",
    " visitNumber          |<p align=\"left\"> The session number for this user. If this is the first session, then this is set to 1\n",
    " visitStartTime       |<p align=\"left\"> The timestamp (expressed as POSIX time)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Useful websites\n",
    "https://pandas.pydata.org/pandas-docs/stable/api.html#flat-file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation\n",
    "* import the usual helpful libs\n",
    "* initialise path names (using multiple locations)\n",
    "* read into newtrain and newtest if available, otherwise train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: ../input/*.csv: No such file or directory\r\n",
      "-rw-r--r--@ 1 colinpearse  staff         7255 25 Oct 21:18 ../input/ga-analytics-with-json-columns/ga_20170601.csv\r\n",
      "-rw-r--r--  1 colinpearse  staff         1157 25 Oct 21:20 ../input/ga-analytics-with-json-columns/ga_20170601_fromtrain.csv\r\n",
      "-rw-r--r--  1 colinpearse  staff      1212669 25 Oct 21:36 ../input/ga-analytics-with-json-columns/ga_20170801_20180430_rev_gt0.csv\r\n",
      "-rw-r--r--  1 colinpearse  staff    222497940 28 Oct 21:32 ../input/ga-analytics-with-json-columns/newtest.csv\r\n",
      "-rw-r--r--  1 colinpearse  staff    244350270 28 Oct 21:31 ../input/ga-analytics-with-json-columns/newtrain.csv\r\n",
      "-rw-r--r--  1 colinpearse  staff     19243532 28 Oct 22:32 ../input/ga-analytics-with-json-columns/submission1.csv\r\n",
      "-rw-r--r--  1 colinpearse  staff     18412503  8 Nov 09:51 ../input/ga-analytics-with-json-columns/submission10.csv\r\n",
      "-rw-r--r--  1 colinpearse  staff     19478856  8 Nov 09:57 ../input/ga-analytics-with-json-columns/submission11.csv\r\n",
      "-rw-r--r--  1 colinpearse  staff     18651683  8 Nov 10:34 ../input/ga-analytics-with-json-columns/submission12.csv\r\n",
      "-rw-r--r--  1 colinpearse  staff     19692216  8 Nov 10:51 ../input/ga-analytics-with-json-columns/submission13.csv\r\n",
      "-rw-r--r--  1 colinpearse  staff     19852875  8 Nov 12:25 ../input/ga-analytics-with-json-columns/submission14.csv\r\n",
      "-rw-r--r--  1 colinpearse  staff     20391204 28 Oct 23:44 ../input/ga-analytics-with-json-columns/submission2.csv\r\n",
      "-rw-r--r--  1 colinpearse  staff     24485101  2 Nov 13:04 ../input/ga-analytics-with-json-columns/submission3.csv\r\n",
      "-rw-r--r--  1 colinpearse  staff     14739065  2 Nov 15:12 ../input/ga-analytics-with-json-columns/submission4.csv\r\n",
      "-rw-r--r--  1 colinpearse  staff     14739065  2 Nov 15:14 ../input/ga-analytics-with-json-columns/submission5.csv\r\n",
      "-rw-r--r--  1 colinpearse  staff     23339798  2 Nov 15:17 ../input/ga-analytics-with-json-columns/submission6.csv\r\n",
      "-rw-r--r--  1 colinpearse  staff     20005240  8 Nov 07:50 ../input/ga-analytics-with-json-columns/submission7.csv\r\n",
      "-rw-r--r--  1 colinpearse  staff     19081894  8 Nov 09:22 ../input/ga-analytics-with-json-columns/submission8.csv\r\n",
      "-rw-r--r--  1 colinpearse  staff     19253347  8 Nov 09:31 ../input/ga-analytics-with-json-columns/submission9.csv\r\n",
      "-rw-r--r--  1 colinpearse  staff     19852875  8 Nov 12:25 ../input/ga-analytics-with-json-columns/submission_lgb.csv\r\n",
      "-rw-r--r--  1 colinpearse  staff    109280100 29 Oct 18:00 ../input/ga-analytics-with-json-columns/testxe.csv\r\n",
      "-rw-r--r--  1 colinpearse  staff    110626590 29 Oct 17:58 ../input/ga-analytics-with-json-columns/trainxe.csv\r\n",
      "-rw-r--r--  1 colinpearse  staff      1391013 29 Oct 17:59 ../input/ga-analytics-with-json-columns/trainxnz.csv\r\n",
      "-rw-r--r--  1 colinpearse  staff      2166215  6 Nov 12:44 ../input/ga-analytics-with-json-columns/validate_realvalues.csv\r\n",
      "-rw-r--r--  1 colinpearse  staff      2174921  6 Nov 12:44 ../input/ga-analytics-with-json-columns/validate_submission.csv\r\n",
      "-rw-r--r--  1 colinpearse  staff     12293013 29 Oct 17:59 ../input/ga-analytics-with-json-columns/validxe.csv\r\n",
      "-rwxr--r--  1 colinpearse  staff      1659589 27 Sep 10:32 \u001b[31m../input/ga-customer-revenue-prediction/fordebug.csv\u001b[m\u001b[m\r\n",
      "-rwxr--r--  1 colinpearse  staff       798974 10 Oct 12:06 \u001b[31m../input/ga-customer-revenue-prediction/json_test.csv\u001b[m\u001b[m\r\n",
      "-rwxr--r--  1 colinpearse  staff     14739065 11 Sep 16:46 \u001b[31m../input/ga-customer-revenue-prediction/sample_submission.csv\u001b[m\u001b[m\r\n",
      "-rwxr--r--  1 colinpearse  staff      7080989  9 Nov 07:41 \u001b[31m../input/ga-customer-revenue-prediction/sample_submission_v2.csv\u001b[m\u001b[m\r\n",
      "-rwxr--r--  1 colinpearse  staff   1346845550 11 Sep 15:39 \u001b[31m../input/ga-customer-revenue-prediction/test.csv\u001b[m\u001b[m\r\n",
      "-rwxr--r--  1 colinpearse  staff      1659843 12 Oct 10:11 \u001b[31m../input/ga-customer-revenue-prediction/test_debug.csv\u001b[m\u001b[m\r\n",
      "-rwxr--r--  1 colinpearse  staff   7615592532  9 Nov 07:41 \u001b[31m../input/ga-customer-revenue-prediction/test_v2.csv\u001b[m\u001b[m\r\n",
      "-rwxr--r--  1 colinpearse  staff   1503430926 11 Sep 15:39 \u001b[31m../input/ga-customer-revenue-prediction/train.csv\u001b[m\u001b[m\r\n",
      "-rwxr--r--  1 colinpearse  staff      1659589 10 Oct 14:45 \u001b[31m../input/ga-customer-revenue-prediction/train_debug.csv\u001b[m\u001b[m\r\n",
      "-rwxr--r--  1 colinpearse  staff  25412027988  9 Nov 07:41 \u001b[31m../input/ga-customer-revenue-prediction/train_v2.csv\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "#pd.set_option('display.max_rows', 500)\n",
    "#pd.set_option('display.max_colwidth', 90)\n",
    "\n",
    "def set_path(paths):\n",
    "    for path in paths:\n",
    "        if os.path.exists(path):\n",
    "            return path\n",
    "\n",
    "#newdata_dir = \".\"\n",
    "newdata_dir = \"../input/ga-analytics-with-json-columns\"\n",
    "data_dir1   = \"../input/ga-customer-revenue-prediction\"\n",
    "data_dir2   = \"../input\"\n",
    "newtrain_path = newdata_dir+\"/newtrain_v2.csv\"\n",
    "newtest_path  = newdata_dir+\"/newtest_v2.csv\"\n",
    "train_path    = set_path([data_dir1+\"/train_v2.csv\", data_dir2+\"/train_v2.csv\"])\n",
    "test_path     = set_path([data_dir1+\"/test_v2.csv\",  data_dir2+\"/test_v2.csv\"])\n",
    "\n",
    "!ls -ld $newdata_dir/*.csv $data_dir1/*.csv $data_dir2/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_new_or_orig(newpath, path):\n",
    "    new = None\n",
    "    orig = None\n",
    "    if os.path.exists(newpath):\n",
    "        new = pd.read_csv(newpath, dtype={'fullVisitorId': 'str', 'visitId': 'str'})\n",
    "        print (\"loaded\",newpath)\n",
    "    elif os.path.exists(path):\n",
    "        orig = pd.read_csv(path, dtype={'fullVisitorId': 'str', 'visitId': 'str'})\n",
    "        print (\"loaded\",path)\n",
    "    else:\n",
    "        print (\"ERROR: loaded nothing\")\n",
    "    return new, orig\n",
    "\n",
    "newtrain, train = load_new_or_orig(newtrain_path, train_path)\n",
    "newtest,  test  = load_new_or_orig(newtest_path, test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic data parsing functions\n",
    "* summary: columns for \"describe\" and default (\"basic\") should be self-explanatory\n",
    "* is_json: not useful at present since I call get_json with specific fields\n",
    "* get_json: apply(json.loads) ensures True/False are ok, but I have to convert strings to NaN explicitly\n",
    "<br>*NOTE: normalize will automatically quote json fields that contain a , (comma) so it's ok to convert the result back to a csv*\n",
    "* expand_json: add new columns using flattened nested json columns, drop duplicates and return a new df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(df, info=\"describe\"):\n",
    "    if info == \"describe\":\n",
    "        headings=['Null','Unique','dType','Type','MinMax','Mean','Std','Skew','Examples']\n",
    "    else:\n",
    "        headings=['Null','Unique','dType','Type','Examples']\n",
    "        \n",
    "    print('DataFrame shape',df.shape)\n",
    "    sdf = pd.DataFrame(index=df.columns, columns=headings)\n",
    "    for col in df.columns:\n",
    "        sdf['Null'][col]     = df[col].isna().sum()\n",
    "        sdf['Unique'][col]   = df[col].astype(str).unique().size\n",
    "        sdf['dType'][col]    = df[col].dtype\n",
    "        sdf['Type'][col]     = \"-\" if df[col].notna().sum() == 0 else type(df[col].dropna().iloc[0])\n",
    "        sdf['Examples'][col] = \"-\" if df[col].notna().sum() == 0 else df[col].astype(str).unique() #.dropna().values\n",
    "        if info == \"describe\":\n",
    "            if 'float' in str(df[col].dtype) or 'int' in str(df[col].dtype):\n",
    "                sdf['MinMax'][col] = str(round(df[col].min(),2))+'/'+str(round(df[col].max(),2))\n",
    "                sdf['Mean'][col]   = df[col].mean()\n",
    "                sdf['Std'][col]    = df[col].std()\n",
    "                sdf['Skew'][col]   = df[col].skew()\n",
    "    return sdf.fillna('-')\n",
    "\n",
    "\n",
    "def is_json(j):\n",
    "    if re.match(r'^{\\\"', j):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_json(df, col):\n",
    "    if is_json(df[col][0]) == False:\n",
    "        return None\n",
    "    jdf_lines = df[col].apply(json.loads)   # do normalize separately or it will use just one column\n",
    "    jdf = pd.io.json.json_normalize(jdf_lines).add_prefix(col+'.')\n",
    "    for jcol in jdf.columns:\n",
    "        jdf[jcol].replace('not available in demo dataset', np.nan, inplace=True, regex=True)\n",
    "        jdf[jcol].replace('(not provided)', np.nan, inplace=True, regex=True)\n",
    "        jdf[jcol].replace('(not set)', np.nan, inplace=True, regex=True)\n",
    "    return jdf\n",
    "\n",
    "def expand_json(df):\n",
    "    newdf = pd.concat([df, get_json(df, 'device'),\n",
    "                           get_json(df, 'geoNetwork'),\n",
    "                           get_json(df, 'totals'),\n",
    "                           get_json(df, 'trafficSource')], axis=1, sort=False)\n",
    "    newdf.drop(columns=['device', 'geoNetwork', 'totals', 'trafficSource'], inplace=True)\n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adhoc validation\n",
    "#get_json(train, 'trafficSource')\n",
    "#get_json(train, 'fullVisitorId')\n",
    "#summary(train, info=\"basic\")\n",
    "#summary(get_json(train, 'device'),       info=\"basic\")\n",
    "#summary(get_json(train, 'geoNetwork'),    info=\"basic\")\n",
    "#summary(get_json(train, 'totals'),        info=\"basic\")\n",
    "#summary(get_json(train, 'trafficSource'), info=\"basic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create wider data frames using nested json entries\n",
    "* create newtrain/newtest and then save them as csvs; they will be smaller since we have removed duplicate json keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_json_to_df(newdf, df):\n",
    "    newname = sys._getframe(1).f_code.co_names[1]  # [0] is the function name\n",
    "    if newdf is None:\n",
    "        print (time.ctime(), newname, \"will contain expanded json entries\")\n",
    "        newdf = expand_json(df)\n",
    "        print (time.ctime(), newname, \"finished: shape =\", newdf.shape, \"vs original shape =\", df.shape)\n",
    "    else:\n",
    "        print (time.ctime(), newname, \"already loaded\")\n",
    "    return newdf\n",
    "\n",
    "newtrain = expand_json_to_df(newtrain, train)\n",
    "newtest  = expand_json_to_df(newtest, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(newtrain, info=\"basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(newtest, info=\"basic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewrite and Reload\n",
    "* if the directory is writable, write newtrain and newtest if they don't already exist\n",
    "* reload back into newtrain and newtest which allows pandas.read_csv to assign more appropriate types (see explanation in heading)\n",
    "<br>*NOTE: python3's default ecoding is \"utf-8\", but explicitly set it so we know what we're getting*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newtrain.csv is only 250MB (from 1.5GB) because we've removed the json repetition\n",
    "# newtest.csv is only 230MB (from 1.3GB) because we've removed the json repetition\n",
    "\n",
    "if os.access(newdata_dir, os.W_OK):\n",
    "    if not os.path.exists(newtrain_path):\n",
    "        newtrain.to_csv(newtrain_path, index=False, encoding=\"utf-8\")\n",
    "        print (\"wrote\", newtrain_path)\n",
    "        newtrain = pd.read_csv(newtrain_path)\n",
    "        print (\"reloaded newtrain\")\n",
    "    if not os.path.exists(newtest_path):\n",
    "        newtest.to_csv(newtest_path, index=False, encoding=\"utf-8\")\n",
    "        print (\"wrote\", newtest_path)\n",
    "        newtest = pd.read_csv(newtest_path)\n",
    "        print (\"reloaded newtest\")\n",
    "else:\n",
    "    print (newdata_dir, \"is not writable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo Quick validation of rows:\n",
    "!sed -n '$=' $newtrain_path\n",
    "!sed -n '$=' $train_path\n",
    "!sed -n '$=' $newtest_path\n",
    "!sed -n '$=' $test_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplifying newtrain and newtest\n",
    "* start with the obvious: if a column has no data for newtrain OR newtest, it is useless to both (we'll drop these)\n",
    "* there two columns not common to both: **totals.transactionRevenue** (we need); **trafficSource.campaignCode** (we'll just ignore this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unused(df):\n",
    "    rows = df.shape[0]\n",
    "    ddf = df.isna().sum()\n",
    "    return list(ddf[ddf >= rows].index)\n",
    "    \n",
    "droplist = get_unused(newtrain)\n",
    "for dropcol in get_unused(newtest):\n",
    "    if dropcol not in droplist:\n",
    "        droplist.append(dropcol)\n",
    "\n",
    "print (\"dropping these columns from both newtrain and newtest:\")\n",
    "print (droplist)\n",
    "newtrain.drop(columns=droplist, inplace=True)\n",
    "newtest.drop(columns=droplist, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(newtrain, info=\"basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(newtest, info=\"basic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA of newtrain data\n",
    "* **totals.transactionRevenue**: NaN vs >0  (imbalance is obvious without a graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zrows = newtrain['totals.transactionRevenue'].size\n",
    "nrows = newtrain['totals.transactionRevenue'].dropna().size\n",
    "print (\"NaN =\", zrows, \"; >0 =\", nrows)\n",
    "#plt.bar([1,2], [zrows, nrows])\n",
    "#plt.ylabel('Rows', fontsize=15)\n",
    "#plt.xticks([1,2], [\"NaN\", \">0\"], fontsize=15, rotation=0)\n",
    "#plt.title(\"totals.transactionRevenue\", fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pie chart functions\n",
    "* col_by_col_count: eg. (df, OS, session) gets session instances counted per OS (so total OS usage)\n",
    "* col_by_col_count: eg. (df, OS, revenue) gets revenue instances counted per OS\n",
    "* col_by_col_sum: eg. (df, OS, revenue) gets revenue total per OS\n",
    "* myautopct: pie is messy with 0.0 and 0.1 percent markings, so set a threshold\n",
    "* mypie: common options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_by_col_count(df, col1, col2, threshold=0):\n",
    "    return df.groupby([col1]).count()[col2].apply(lambda x: (x if x>threshold else np.nan)).dropna()\n",
    "\n",
    "def col_by_col_sum(df, col1, col2, threshold=0):\n",
    "    return df.groupby([col1]).sum(numeric_only=True)[col2].apply(lambda x: (x if x>threshold else np.nan)).dropna()\n",
    "\n",
    "def myautopct(pct):\n",
    "    return ('%.2f' % pct) if pct > 2 else ''\n",
    "\n",
    "def mypie(df, title, angle=0):\n",
    "    # autopct='%1.1f%%'\n",
    "    # textprops={'size': 'small'}  (Kaggle python (3.6.6) + libs didn't recognise this)\n",
    "    df.plot(kind='pie', figsize=(5, 5), radius=1.2, startangle=angle, autopct=myautopct, pctdistance=0.8,\n",
    "        rotatelabels=False, legend=True, explode=[0.02]*df.size);\n",
    "    plt.title(title, weight='bold', size=14, x=2.0, y=-0.01);\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "    plt.legend(bbox_to_anchor=(2.5, 1.0), ncol=2, fontsize=10, fancybox=True, shadow=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pie deductions\n",
    "* OSs: 19 in total, and we've removed negligable sales on Nintendo etc.\n",
    "* OS split: Windows the clear winner, no suprises there. 10 OSs below the threshold and so not represented here.\n",
    "* OS by transaction instance: people buy more on a Mac it seems\n",
    "* OS by transaction sum: more or less the above, except people spend bigger amounts when using Chrome OS and Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newtrain['device.operatingSystem'].astype(str).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = col_by_col_count(newtrain, 'device.operatingSystem', 'sessionId', threshold=100)\n",
    "mypie(df, 'OS prevalence', angle=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = col_by_col_count(newtrain, 'device.operatingSystem', 'totals.transactionRevenue', threshold=100)\n",
    "mypie(df, 'OS prevalence by revenue instances', angle=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = col_by_col_sum(newtrain, 'device.operatingSystem', 'totals.transactionRevenue', threshold=100)\n",
    "mypie(df, 'OS prevalence by revenue sum', angle=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
